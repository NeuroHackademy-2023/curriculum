{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dce6adc",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing intracranial EEG using MNE-python - Epochs!\n",
    "\n",
    "*NeuroHackademy 2023*  \n",
    "[Liberty Hamilton, PhD](https://slhs.utexas.edu/research/hamilton-lab)  \n",
    "Assistant Professor, Department of Speech, Language, and Hearing Sciences and  \n",
    "Department of Neurology  \n",
    "The University of Texas at Austin  \n",
    "\n",
    "This is part two of the notebooks. Please first run through [`01_ieeg_preprocessing_MNE.ipynb`](01_ieeg_preprocessing_MNE.ipynb) before running this. In this portion of the tutorial, you will learn about epoching your data. Epoched data allows you to calculate averaged responses to events of interest (event-related potentials). We will do this based on the provided annotations of speech vs. music, as well as additional annotations that are available in the Berezutskaya dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mne_bids import read_raw_bids\n",
    "from mne_bids.path import get_bids_path_from_fname\n",
    "from bids import BIDSLayout\n",
    "from ecog_preproc_utils import transformData\n",
    "import bids "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f2b5d",
   "metadata": {},
   "source": [
    "## Load BIDS iEEG dataset\n",
    "\n",
    "Here we will load an example iEEG dataset from [Berezutskaya et al.  Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film](https://openneuro.org/datasets/ds003688/versions/1.0.7/metadata). For this tutorial we will use data from `sub-06`, `iemu` data only, which has been downloaded to the jupyter hub. The whole dataset is rather large (15 GB), so if you prefer to download just this session you can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the example participant's data that we will load for the tutorial,\n",
    "# but there are more options.\n",
    "subj = '06'\n",
    "sess = 'iemu'\n",
    "task = 'film'\n",
    "acq = 'clinical'\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data directory below to where your data are located. \n",
    "parent_dir = '/home/jovyan/shared/ds003688/'  # This is on the jupyter hub\n",
    "ieeg_dir = f'{parent_dir}/sub-{subj}/ses-{sess}/ieeg/'\n",
    "channel_path = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_acq-{acq}_run-{run}_channels.tsv'\n",
    "raw_path = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_acq-{acq}_run-{run}_ieeg.vhdr'\n",
    "\n",
    "bids_path = get_bids_path_from_fname(raw_path)\n",
    "base_name = os.path.basename(raw_path).split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da51ef",
   "metadata": {},
   "source": [
    "## Load the iEEG data\n",
    "\n",
    "First, we will choose the relevant subject, session, task, acquisition, and run. Note that if you wish to change these variables, you may need to download the data yourself.\n",
    "\n",
    "To show the capabilities of BIDS and contrast to when we don't use BIDS, we'll load the data in two ways. The data structure using BIDS will be called `raw`, the data structure without BIDS will be `raw_nobids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and extract parameters from BIDS files\n",
    "raw = read_raw_bids(bids_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f183f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data into memory and print some information about it. The \n",
    "# info structure contains a lot of helpful metadata about number of channels,\n",
    "# sampling rate, data types, etc. It can also contain information about the\n",
    "# participant and date of acquisition, however, this dataset has been anonymized.\n",
    "raw.load_data()\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbb278",
   "metadata": {},
   "source": [
    "# Calculate the high gamma transform of your data\n",
    "\n",
    "Now we will take the raw, preprocessed data, and convert to high gamma analytic amplitude for further analysis. The high gamma analytic amplitude is used in many papers as a proxy for multi-unit firing (see [Ray and Maunsell, PLoS Biology 2011](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000610)).\n",
    "\n",
    "This particular version of the high gamma transform uses the same procedure as used in [Hamilton et al. 2018](cell.com/current-biology/pdf/S0960-9822(18)30461-5.pdf) and [Hamilton et al. 2021](https://www.cell.com/cell/pdf/S0092-8674(21)00878-3.pdf). The basic idea is to take 8 bands within the 70-150 Hz range, calculate the Hilbert transform, then take the analytic amplitude of that signal and average across the 8 bands. This form of averaging results in higher SNR than one band between 70-150 Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the iEEG channels for high gamma\n",
    "raw_ieeg = raw.copy()\n",
    "raw_ieeg.pick_types(ecog=True)\n",
    "raw_ieeg.anonymize()\n",
    "\n",
    "notch_freqs = list(np.arange(raw.info['line_freq'], raw.info['lowpass'], step=raw.info['line_freq']))\n",
    "# Get the high gamma data\n",
    "# Generally, do a CAR if you have widespread coverage over multiple\n",
    "# areas (not just one sensory area)\n",
    "# If you have limited coverage, you may choose to do no CAR or choose\n",
    "# to reference to one specific channel.\n",
    "hgdat = transformData(raw_ieeg, ieeg_dir, band='high_gamma', notch=True, CAR=True,\n",
    "                      car_chans='average', log_transform=True, do_zscore=True,\n",
    "                      hg_fs=100, notch_freqs=notch_freqs, overwrite=True,\n",
    "                      ch_types='ecog')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d961f2",
   "metadata": {},
   "source": [
    "# Plotting evoked data\n",
    "\n",
    "Now that we have some preprocessed data, let's plot the differences between experimental conditions. To do this, we will need the events timings, which are included in the `events.tsv` file. In this case, the events correspond to blocks of music and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc5c0e",
   "metadata": {},
   "source": [
    "## Loading events\n",
    "\n",
    "Now we will load events from the .tsv file to plot evoked responses to music and speech events. First I'll show you how to do this by creating an MNE events array, next I'll show you how to derive them from the annotations. This first method could also be used with non-BIDS datasets if you have the onset and duration and trial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397699df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple way of loading a tab-delimited file, and is not specific to\n",
    "# MNE python. We're using the library pandas, which you may also find very\n",
    "# helpful in other applications.\n",
    "event_file = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_run-{run}_events.tsv'\n",
    "event_df = pd.read_csv(event_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the contents of this dataframe. \n",
    "event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bdf27",
   "metadata": {},
   "source": [
    "## Convert event times to samples\n",
    "\n",
    "Now these event times are in seconds, not samples, so we have to convert them for use with MNE python's epochs constructor. Let's do that here. \n",
    "\n",
    "The times here are in seconds, and sampling rate is in units of Hz (samples/sec), so to get samples, we just multiply the amount of time by the sampling rate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mbox{number of samples} &=& \\mbox{time }  \\times \\mbox{sampling rate}\\\\\n",
    "\\mbox{(samples)} &=& \\mbox{(s) }  \\times \\mbox{(samples/sec)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We also cast these as integers since data samples are discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_samp = [int(onset*hgdat.info['sfreq']) for onset in event_df.onset]\n",
    "dur_samp = [int(dur*hgdat.info['sfreq']) for dur in event_df.duration]\n",
    "ev_id = [int(e*hgdat.info['sfreq']) for e in event_df.value]\n",
    "\n",
    "eve = list(zip(onset_samp, dur_samp, ev_id))\n",
    "eve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996b3d3",
   "metadata": {},
   "source": [
    "## Another way...\n",
    "\n",
    "So actually, because we already had these particular events as annotations, we could have also done this a simpler way, but the method above also works for other events that are stored in tsv files without becoming annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also do this with the `raw` object\n",
    "events = mne.events_from_annotations(hgdat, event_id='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd819c75",
   "metadata": {},
   "source": [
    "## Create an epochs object\n",
    "\n",
    "Now if we want to plot our data by epoch type, we can use the mne Epochs class. This allows us to parse our data according to these events and plot evoked activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ed235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -0.2  # How much time to account for before the event of interest\n",
    "tmax = 0.5   # How much time to account for after the event of interest\n",
    "event_id = events[1]['speech']  # This is the speech event ID\n",
    "\n",
    "# Here we take events[0] because those are the timings, whereas events[1] has\n",
    "# the information about event type. If you just have a list of timings,\n",
    "# you don't need to index the events in this way.\n",
    "epochs = mne.Epochs(hgdat, events=events[0], tmin=tmin, tmax=tmax, event_id=event_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will just plot the average across all channels. This is a bit\n",
    "# weird to do with iEEG because this is across a lot of different brain\n",
    "# areas, but it's still possible.\n",
    "epochs.plot_image(combine='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about plotting a particular electrode? This is one that\n",
    "# appears to be on the STG based on the image above\n",
    "epochs.plot_image(picks=[hgdat.info['ch_names'][13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90528d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(epochs, nchans, ch_names, color='b', label='spkr', show=True, vmin_max=None):\n",
    "    '''\n",
    "    Function that plots the averaged epoched data for each channel as a grid so you can \n",
    "    see all channels at once.\n",
    "    \n",
    "    Inputs:\n",
    "        epochs [obj] : MNE epochs object\n",
    "        nchans [int] : number of channels to plot\n",
    "        ch_names [list] : channel names \n",
    "        color [str, hex, tuple]: color for ERP traces\n",
    "        label [str] : label for the ERP (could be epoch type/annotation type) \n",
    "        show [bool] : whether to show the figure or not\n",
    "        vmin_max [list] : list of ylim min and max, e.g. [-0.5, 0.5]\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Get the data as an array\n",
    "    eps = epochs.get_data()\n",
    "    \n",
    "    # Find the maximum across the whole dataset, helps with scaling the plots\n",
    "    emax = np.abs(epochs.average().data).max()\n",
    "    \n",
    "    # Determine how many rows and columns we'll need in our subplots grid\n",
    "    # based on the number of channels. \n",
    "    nrows = int(np.floor(np.sqrt(nchans)))\n",
    "    ncols = int(np.ceil(nchans/nrows))\n",
    "    \n",
    "    # Loop through all electrode channels\n",
    "    for ch in np.arange(nchans):\n",
    "        plt.subplot(nrows, ncols, ch+1)\n",
    "        \n",
    "        # Get the average response across trials for this particular channel\n",
    "        erp = eps[:,ch,:].mean(0)\n",
    "        \n",
    "        # Get the standard error across trials\n",
    "        erpstderr = eps[:,ch,:].std(0)/np.sqrt(eps.shape[0])\n",
    "        \n",
    "        # Plot transparent shaded standard error in the [color] you choose\n",
    "        ybottom = erp - erpstderr\n",
    "        ytop = erp + erpstderr\n",
    "        plt.fill_between(epochs.times, ybottom.ravel(), ytop.ravel(),\n",
    "                         alpha=0.5, color=color)\n",
    "        \n",
    "        # Plot the average epoch on top in the same color\n",
    "        plt.plot(epochs.times, erp, color=color, label=label)\n",
    "        \n",
    "        # Plot the x and y origins\n",
    "        plt.axvline([0], color='k', linewidth=0.5)\n",
    "        plt.axhline([0], color='k', linewidth=0.5)\n",
    "        \n",
    "        # If we haven't explicitly set ylimits with vmin/vmax, use \n",
    "        # the maximum of the data and 50% more so the whole thing \n",
    "        # fits nicely \n",
    "        if vmin_max is None:\n",
    "            plt.gca().set_ylim([-emax*1.5, emax*1.5])\n",
    "        else:\n",
    "            plt.gca().set_ylim([vmin_max[0], vmin_max[1]])\n",
    "            \n",
    "        # Only show the ticks for the 0th plot, otherwise this gets\n",
    "        # hard to see/read\n",
    "        if ch != 0:\n",
    "            plt.gca().set_xticks([])\n",
    "            plt.gca().set_yticks([])\n",
    "        else:\n",
    "            plt.ylabel('Z-score')\n",
    "        \n",
    "        # Write the name of the channel in the plot -- you could also\n",
    "        # use plt.title() but sometimes that makes everything look\n",
    "        # a little squashed\n",
    "        plt.text(0.5, 0.25, ch_names[ch], \n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            transform=plt.gca().transAxes, fontsize=8)\n",
    "    \n",
    "    # Plot ticks at meaningful times (the min, 0, and max in seconds)\n",
    "    plt.gca().set_xticks([epochs.tmin, 0, epochs.tmax])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.legend()\n",
    "    #plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_epochs(epochs, len(hgdat.info['ch_names']), hgdat.info['ch_names'], label='speech', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db262249",
   "metadata": {},
   "source": [
    "## Using stimulus annotations\n",
    "\n",
    "In addition to the \"speech\" vs \"music\" gross-level annotations, the researchers have provided information about the onset and offset of different types of information in the sound as well as the video. You can look in the `stimuli` folder to see what types of annotations are provided, but in general, these include word-level, syllable-level, sentence-level, and specific talkers as well as some other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word times\n",
    "annotation = 'words'  # try other types of annotations here! \n",
    "word_times = pd.read_csv(f'{parent_dir}/stimuli/annotations/sound/sound_annotation_{annotation}.tsv', delimiter='\\t')\n",
    "\n",
    "# print them\n",
    "word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ef844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the sample that corresponds to the start of the task, since we\n",
    "# will need to offset all the stimulus time labels from that\n",
    "start_sample = events[0][events[0][:,2] == events[1]['start task'],0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82474e0",
   "metadata": {},
   "source": [
    "### Create the new word events epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_events = []\n",
    "\n",
    "# Loop through the times for each word event and convert to samples, as we did before\n",
    "# This time we can't use annotations, because these word events were not included\n",
    "# as annotations in the raw files, just as .tsv files.\n",
    "for idx, row in word_times.iterrows():\n",
    "    onset_sample = int(row['onset']*hgdat.info['sfreq'])  # convert time to samples\n",
    "    offset_sample = int(row['offset']*hgdat.info['sfreq'])  # convert time to samples\n",
    "    duration_sample = offset_sample - onset_sample  # Get the duration in samples\n",
    "    onset_sample += start_sample  # need to shift by the actual starting time of the task\n",
    "    \n",
    "    # Append this event to our events list\n",
    "    word_events.append([onset_sample, duration_sample, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35595f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the epochs object again. Note that we don't need to index the `word_events`\n",
    "# because it is already a list in the correct format\n",
    "epochs_words = mne.Epochs(hgdat, events=word_events, tmin=-0.2, tmax=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0941a7b",
   "metadata": {},
   "source": [
    "### Plot word epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average across all channels\n",
    "epochs_words.plot_image(combine='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e769831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one subplot for each channel as we had done before\n",
    "plot_epochs(epochs_words, len(hgdat.info['ch_names']), hgdat.info['ch_names'], label='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one example electrode that has a strong word response\n",
    "epochs_words.plot_image(picks=['P18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e4c57",
   "metadata": {},
   "source": [
    "### Export data to numpy array\n",
    "\n",
    "If we want to export the data to use with our own functions, we can also do that with the `.get_data()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from our epochs_words object to do other things with\n",
    "epochs_array = epochs_words.get_data()\n",
    "print(f'{epochs_array.shape[0]} {annotation} events for\\\n",
    "  {epochs_array.shape[1]} channels and {epochs_array.shape[2]} time points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matplotlib to show the average across all epochs\n",
    "# Scale to the -max(abs) of the data and max(abs) of the data\n",
    "# with a diverging colormap so that the color for 0 is white,\n",
    "# and positive values are red, and negative values are blue\n",
    "plt.imshow(epochs_array.mean(0), cmap=cm.RdBu_r, \n",
    "           vmin=-np.max(np.abs(epochs_array.mean(0))),\n",
    "           vmax=np.max(np.abs(epochs_array.mean(0))))  # Take the average across all trials (words)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Channel')\n",
    "plt.gca().set_xticks([0, \n",
    "                      -int(epochs_words.tmin*epochs_words.info['sfreq']), \n",
    "                      int((epochs_words.tmax - epochs_words.tmin)*epochs_words.info['sfreq'])])\n",
    "plt.gca().set_xticklabels([epochs_words.tmin, 0, epochs_words.tmax])\n",
    "plt.axvline(-int(epochs_words.tmin*epochs_words.info['sfreq']), color='k', linestyle='--')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93b531",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "Some suggestions for things you can try:\n",
    "\n",
    "* Create epochs for different types of events - speech, music, syllables, sentences, etc\n",
    "* Compare amplitude of speech versus music responses in each electrode. Note that you can use `show=False` and call `plot_epochs` more than once to plot different epochs on the same axes\n",
    "* Look at effects of referencing on the evoked data/epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
