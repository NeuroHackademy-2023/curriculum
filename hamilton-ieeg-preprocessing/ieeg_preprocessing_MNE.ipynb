{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dce6adc",
   "metadata": {},
   "source": [
    "# Preprocessing intracranial EEG using MNE-python\n",
    "\n",
    "*NeuroHackademy 2023*  \n",
    "[Liberty Hamilton, PhD](https://slhs.utexas.edu/research/hamilton-lab)  \n",
    "Assistant Professor, Department of Speech, Language, and Hearing Sciences and  \n",
    "Department of Neurology  \n",
    "The University of Texas at Austin  \n",
    "\n",
    "This notebook will show you how to preprocess intracranial EEG using MNE-python. This uses a freely available iEEG dataset on audiovisual movie watching from [Julia Berezutskaya, available on OpenNeuro.org](https://openneuro.org/datasets/ds003688/versions/1.0.7/metadata). This notebook mostly covers the basics of how to look at iEEG data, and my tutorial will discuss how to find and identify artifacts. The method of high gamma extraction is identical to that used in [Hamilton et al. 2018](https://doi.org/10.1016/j.cub.2018.04.033) and [Hamilton et al. 2021](https://doi.org/10.1016/j.cell.2021.07.019).\n",
    "\n",
    "## Python libraries used in this tutorial\n",
    "\n",
    "* MNE-python\n",
    "* numpy\n",
    "* pandas\n",
    "* matplotlib\n",
    "\n",
    "## What you will do in this tutorial\n",
    "\n",
    "* Load an iEEG dataset in MNE-python\n",
    "* Compare iEEG dataset with BIDs metadata vs. without so you know what to do if you encounter data without this info\n",
    "* Plot the power spectrum of the data to check for bad channels and compare channel types\n",
    "* Re-reference the data according to different reference schemes\n",
    "* Compute the high gamma analytic amplitude of the signal\n",
    "* Plot evoked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "from mne_bids.path import get_bids_path_from_fname\n",
    "from bids import BIDSLayout\n",
    "from ecog_preproc_utils import transformData\n",
    "import bids "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f2b5d",
   "metadata": {},
   "source": [
    "## Download BIDS iEEG dataset\n",
    "\n",
    "Here we will download an example iEEG dataset from [Berezutskaya et al.  Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film](https://openneuro.org/datasets/ds003688/versions/1.0.7/metadata). For this tutorial we will use data from `sub-06`, `iemu` data only, which has been downloaded to the jupyter hub. The whole dataset is rather large (15 GB), so if you prefer to download just this session you can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2655a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data directory below to where your data are located. \n",
    "parent_dir = '/home/jovyan/shared/ds003688/'  # This is on the jupyter hub\n",
    "ieeg_dir = f'{parent_dir}/sub-{subj}/ses-{sess}/ieeg/'\n",
    "channel_path = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_acq-{acq}_run-{run}_channels.tsv'\n",
    "raw_path = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_acq-{acq}_run-{run}_ieeg.vhdr'\n",
    "\n",
    "bids_path = get_bids_path_from_fname(raw_path)\n",
    "base_name = os.path.basename(raw_path).split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9faf41",
   "metadata": {},
   "source": [
    "## BIDS layout\n",
    "\n",
    "We can use `pybids` to show a little bit about the files in this BIDS dataset. We won't get as much into this, but if you'd like to try this tutorial on your own you may wish to delve into this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953df68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d328249",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = layout.get()\n",
    "print(\"There are {} files in the layout.\".format(len(all_files)))\n",
    "print(\"\\nThe first 10 files are:\")\n",
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0587e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(parent_dir, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da51ef",
   "metadata": {},
   "source": [
    "## Let's load some iEEG data!\n",
    "\n",
    "First, we will choose the relevant subject, session, task, acquisition, and run. Note that if you wish to change these variables, you may need to download the data yourself.\n",
    "\n",
    "To show the capabilities of BIDS and contrast to when we don't use BIDS, we'll load the data in two ways. The data structure using BIDS will be called `raw`, the data structure without BIDS will be `raw_nobids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables to work for your block\n",
    "subj = '06'\n",
    "sess = 'iemu'\n",
    "task = 'film'\n",
    "acq = 'clinical'\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and extract parameters from BIDS files\n",
    "raw = read_raw_bids(bids_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f070387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data assuming we didn't have the BIDS structure in place\n",
    "raw_nobids = mne.io.read_raw_brainvision(raw_path, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f183f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data into memory and print some information about it. The \n",
    "# info structure contains a lot of helpful metadata about number of channels,\n",
    "# sampling rate, data types, etc. It can also contain information about the\n",
    "# participant and date of acquisition, however, this dataset has been anonymized.\n",
    "raw.load_data()\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info['ch_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f25b19",
   "metadata": {},
   "source": [
    "## Plot the power spectrum\n",
    "\n",
    "Now we will plot the power spectrum of the signal to give us an idea of the signals we're getting. Bad channels (or channels that are not EEG/ECoG) will often have a very different power spectrum than the good channels. These will show up as highly outside the range of the other channels (either flat, or much higher/lower power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.compute_psd().plot(picks='data', exclude=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19780bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to see other options we have for computing the power spectrum, \n",
    "# we can consult the help function\n",
    "raw.compute_psd?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c9dc",
   "metadata": {},
   "source": [
    "## Do the same without having loaded the metadata from BIDs\n",
    "\n",
    "Here we will see the data with bad channels included, and with all the channel types marked as EEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nobids.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27232b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nobids.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data, reject bad segments. Look for times where there\n",
    "# are spike wave discharges (epileptiform artifacts) or large\n",
    "# movement artifacts. Be selective, look out for blocks with a \n",
    "# ton of seizure activity\n",
    "raw.plot(scalings='auto', color=dict(eeg='b', ecog='b'), n_channels=64, block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd98ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data, reject bad segments. Look for times where there\n",
    "# are spike wave discharges (epileptiform artifacts) or large\n",
    "# movement artifacts. Be selective, look out for blocks with a \n",
    "# ton of seizure activity. This is when we don't have the nice\n",
    "# BIDS metadata automatically loaded in.\n",
    "raw_nobids.plot(scalings='auto', color=dict(eeg='b', ecog='b'), n_channels=64, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6e07d",
   "metadata": {},
   "source": [
    "## Referencing\n",
    "\n",
    "Referencing or re-referencing your data should be done with some knowledge of your recording setup and what you wish to measure. You can read more about referencing [here (for EEG)](https://pressrelease.brainproducts.com/referencing/#:~:text=The%20reference%20influences%20the%20amplitude,affected%20by%20similar%20electrical%20activity.). Typically, experimenters will choose one of the following references:\n",
    "\n",
    "1. Based on a single electrode in white matter (or relatively \"quiet\" electrode far away from your signals of interest. \n",
    "2. Based on the average of all electrodes or a block of electrodes (CAR or Common Average Reference). Note that the CAR is *not* a good idea if all of your electrodes are within a single functional area, as you will likely subtract out more signal than noise. \n",
    "3. Bipolar referencing, in which pairs of adjacent electrodes are subtracted to calculate more local signals. This is a bit more complicated but allows you to work with data in a single region without the drawbacks of the CAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of single reference channel - this subtracts the signal from this channel\n",
    "# from all other channels (including itself), so the reference will appear as a flat\n",
    "# line after this step\n",
    "raw_ref = raw.copy()\n",
    "raw_ref.set_eeg_reference(ref_channels = ['P01'], )\n",
    "raw_ref.plot(scalings='auto', color=dict(eeg='b', ecog='b'), n_channels=64, block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3213ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of common average reference. The common average reference subtracts the average\n",
    "# signal across all *good* channels from every single channel. This is typically a good\n",
    "# choice for removing noise across all channels (for example, sometimes EMG from chewing,\n",
    "# some movement artifacts, electrical noise).\n",
    "raw_ref_car = raw.copy()\n",
    "raw_ref_car.set_eeg_reference(ref_channels = 'average')\n",
    "raw_ref_car.plot(scalings='auto', color=dict(eeg='b', ecog='b'), n_channels=64, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95cd07",
   "metadata": {},
   "source": [
    "## Bipolar reference\n",
    "\n",
    "Bipolar referencing is a bit trickier and is not fully implemented here. You need to use knowledge of the physical locations of the electrodes to properly create the bipolar montage. For example, in the image below, we would need to use the knowledge of how the electrodes are placed in order to create the appropriate pairs for the anode and cathode.\n",
    "\n",
    "![sub-06 electrode locations](sub-06_ses-iemu_acq-render_photo_ecog_left.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of bipolar reference. This would normally be done with some manual\n",
    "# intervention of the specific channel pairs. \n",
    "import re  # regex for comparing channel names\n",
    "\n",
    "raw_ieeg = raw.copy()\n",
    "raw_ieeg.pick_types(ecog=True)\n",
    "ch_pairs = list(zip(raw_eeg.info['ch_names'][:-1],\n",
    "                    raw_eeg.info['ch_names'][1:]))\n",
    "\n",
    "# Make a list of channels for the anode and the cathode\n",
    "anode = []\n",
    "cathode = []\n",
    "# Only subtract channels with the same device name (these will be\n",
    "# close in space). This is still not ideal as we are probably\n",
    "# subtracting electrodes that are far away from one another in \n",
    "# space (for example, electrodes 8 and 9 on the grid picture\n",
    "# above should not be used for a bipolar reference)\n",
    "for pair in ch_pairs:\n",
    "    # get rid of the numbers in the ch_name\n",
    "    ch1_dev = re.sub(r'\\d+', '', pair[0]) \n",
    "    ch2_dev = re.sub(r'\\d+', '', pair[1]) \n",
    "    # if these are part of the same device, consider them for \n",
    "    # anode and cathode selection\n",
    "    if ch1_dev == ch2_dev:\n",
    "        anode.append(pair[0])\n",
    "        cathode.append(pair[1])\n",
    "\n",
    "# Apply the bipolar reference\n",
    "raw_ref_bip = mne.set_bipolar_reference(raw, anode=anode, cathode=cathode)\n",
    "raw_ref_bip.plot(scalings='auto', color=dict(eeg='b', ecog='b'), n_channels=64, block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbb278",
   "metadata": {},
   "source": [
    "# Calculate the high gamma transform of your data\n",
    "\n",
    "Now we will take the raw, preprocessed data, and convert to high gamma analytic amplitude for further analysis. The high gamma analytic amplitude is used in many papers as a proxy for multi-unit firing (see [Ray and Maunsell, PLoS Biology 2011](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000610)).\n",
    "\n",
    "This particular version of the high gamma transform uses the same procedure as used in [Hamilton et al. 2018](cell.com/current-biology/pdf/S0960-9822(18)30461-5.pdf) and [Hamilton et al. 2021](https://www.cell.com/cell/pdf/S0092-8674(21)00878-3.pdf). The basic idea is to take 8 bands within the 70-150 Hz range, calculate the Hilbert transform, then take the analytic amplitude of that signal and average across the 8 bands. This form of averaging results in higher SNR than one band between 70-150 Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the high gamma data\n",
    "# Generally, do a CAR if you have widespread coverage over multiple\n",
    "# areas (not just one sensory area)\n",
    "# If you have limited coverage, you may choose to do no CAR or choose\n",
    "# to reference to one specific channel.\n",
    "hgdat = transformData(raw_ieeg, ieeg_dir, band='high_gamma', notch=True, CAR=True,\n",
    "                      car_chans='average', log_transform=True, do_zscore=True,\n",
    "                      hg_fs=100, notch_freqs=[60,120,180], overwrite=False,\n",
    "                      ch_types='ecog', save=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d961f2",
   "metadata": {},
   "source": [
    "# Plotting evoked data\n",
    "\n",
    "Now that we have some preprocessed data, let's plot the differences between experimental conditions. To do this, we will need the events timings, which are included in the `events.tsv` file. In this case, the events correspond to blocks of music and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc5c0e",
   "metadata": {},
   "source": [
    "## Loading events\n",
    "\n",
    "Now we will load events from the .tsv file to plot evoked responses to music and speech events. First I'll show you how to do this by creating an MNE events array, next I'll show you how to derive them from the annotations. This first method could also be used with non-BIDS datasets if you have the onset and duration and trial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397699df",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_file = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_run-{run}_events.tsv'\n",
    "event_df = pd.read_csv(event_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bdf27",
   "metadata": {},
   "source": [
    "## Convert event times to samples\n",
    "\n",
    "Now these event times are in seconds, not samples, so we have to convert them for use with MNE python's epochs constructor. Let's do that here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_samp = [int(onset*hgdat.info['sfreq']) for onset in event_df.onset]\n",
    "dur_samp = [int(dur*hgdat.info['sfreq']) for dur in event_df.duration]\n",
    "ev_id = [int(e*hgdat.info['sfreq']) for e in event_df.value]\n",
    "\n",
    "eve = list(zip(onset_samp, dur_samp, ev_id))\n",
    "print(eve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996b3d3",
   "metadata": {},
   "source": [
    "## Another way...\n",
    "\n",
    "So actually, because we already had these particular events as annotations, we could have also done this a simpler way, but the method above also works for other events that are stored in tsv files without becoming annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.events_from_annotations(hgdat, event_id='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd819c75",
   "metadata": {},
   "source": [
    "## Create an epochs object\n",
    "\n",
    "Now if we want to plot our data by epoch type, we can use the mne Epochs class. This allows us to parse our data according to these events and plot evoked activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ed235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -0.2  # How much time to account for before the event of interest\n",
    "tmax = 0.5   # How much time to account for after the event of interest\n",
    "event_id = [3]  # This is the speech event ID\n",
    "epochs = mne.Epochs(hgdat, events=events[0], tmin=tmin, tmax=tmax, event_id=event_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will just plot the average\n",
    "epochs.plot_image(combine='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about plotting a particular electrode?\n",
    "epochs.plot_image(picks=[hgdat.info['ch_names'][13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90528d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(epochs, nchans, ch_names, color='b', label='spkr', show=True, vmin_max=None):\n",
    "    '''\n",
    "    Function that plots the averaged epoched data for each channel as a grid so you can \n",
    "    see all channels at once.\n",
    "    \n",
    "    Inputs:\n",
    "        epochs [obj] : MNE epochs object\n",
    "        nchans [int] : number of channels to plot\n",
    "        ch_names [list] : channel names \n",
    "        color [str, hex, tuple]: color for ERP traces\n",
    "        label [str] : label for the ERP (could be epoch type/annotation type) \n",
    "        show [bool] : whether to show the figure or not\n",
    "        vmin_max [list] : list of ylim min and max, e.g. [-0.5, 0.5]\n",
    "        \n",
    "    '''\n",
    "    eps = epochs.get_data()\n",
    "    emax = np.abs(epochs.average().data).max()\n",
    "    nrows = int(np.floor(np.sqrt(nchans)))\n",
    "    ncols = int(np.ceil(nchans/nrows))\n",
    "    for ch in np.arange(nchans):\n",
    "        plt.subplot(nrows, ncols, ch+1)\n",
    "        erp = eps[:,ch,:].mean(0)\n",
    "        erpstd = eps[:,ch,:].std(0)/np.sqrt(eps.shape[0])\n",
    "        ybottom = erp - erpstd\n",
    "        ytop = erp + erpstd\n",
    "\n",
    "        plt.fill_between(epochs.times, ybottom.ravel(), ytop.ravel(),\n",
    "                         alpha=0.5, color=color)\n",
    "        plt.plot(epochs.times, erp, color=color, label=label)\n",
    "        plt.axvline([0], color='k', linewidth=0.5)\n",
    "        plt.axhline([0], color='k', linewidth=0.5)\n",
    "        if vmin_max is None:\n",
    "            plt.gca().set_ylim([-emax*1.5, emax*1.5])\n",
    "        else:\n",
    "            plt.gca().set_ylim([vmin_max[0], vmin_max[1]])\n",
    "        if ch != 0:\n",
    "            plt.gca().set_xticks([])\n",
    "            plt.gca().set_yticks([])\n",
    "        else:\n",
    "            plt.ylabel('Z-score')\n",
    "\n",
    "        plt.text(0.5, 0.25, ch_names[ch], \n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            transform=plt.gca().transAxes, fontsize=8)\n",
    "    plt.gca().set_xticks([epochs.tmin, 0, epochs.tmax])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.legend()\n",
    "    #plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs(epochs, len(hgdat.info['ch_names']), hgdat.info['ch_names'], label='speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db262249",
   "metadata": {},
   "source": [
    "## Using stimulus annotations\n",
    "\n",
    "In addition to the \"speech\" vs \"music\" gross-level annotations, the researchers have provided information about the onset and offset of different types of information in the sound as well as the video. You can look in the `stimuli` folder to see what types of annotations are provided, but in general, these include word-level, syllable-level, sentence-level, and specific talkers as well as some other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word times\n",
    "annotation = 'words'  # try others here! \n",
    "word_times = pd.read_csv(f'{parent_dir}/stimuli/annotations/sound/sound_annotation_{annotation}.tsv', delimiter='\\t')\n",
    "word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ef844",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sample = events[0][events[0][:,2] == events[1]['start task'],0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82474e0",
   "metadata": {},
   "source": [
    "### Create the new word events epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_events = []\n",
    "for idx, row in word_times.iterrows():\n",
    "    onset_sample = int(row['onset']*hgdat.info['sfreq'])  # convert time to samples\n",
    "    offset_sample = int(row['offset']*hgdat.info['sfreq'])  # convert time to samples\n",
    "    duration_sample = offset_sample - onset_sample\n",
    "    onset_sample+= start_sample  # need to shift by the actual starting time of the task\n",
    "    \n",
    "    word_events.append([onset_sample, duration_sample, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35595f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_words = mne.Epochs(hgdat, events=word_events, tmin=-0.2, tmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0941a7b",
   "metadata": {},
   "source": [
    "### Plot word epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_words.plot_image(combine='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e769831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs(epochs_words, len(hgdat.info['ch_names']), hgdat.info['ch_names'], label='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_words.plot_image(picks=['P18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e4c57",
   "metadata": {},
   "source": [
    "### Export data to numpy array\n",
    "\n",
    "If we want to export the data to use with our own functions, we can also do that with the `.get_data()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_array = epochs_words.get_data()\n",
    "print(f'{epochs_array.shape[0]} {annotation} events for\\\n",
    "  {epochs_array.shape[1]} channels and {epochs_array.shape[2]} time points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matplotlib to show the average across all epochs\n",
    "plt.imshow(epochs_array.mean(0))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Channel')\n",
    "plt.gca().set_xticks([0, \n",
    "                      -int(epochs_words.tmin*epochs_words.info['sfreq']), \n",
    "                      int((epochs_words.tmax - epochs_words.tmin)*epochs_words.info['sfreq'])])\n",
    "plt.gca().set_xticklabels([epochs_words.tmin, 0, epochs_words.tmax])\n",
    "plt.axvline(-int(epochs_words.tmin*epochs_words.info['sfreq']), color='w', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_words.info['sfreq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93b531",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "Some suggestions for things you can try:\n",
    "\n",
    "* Create epochs for different types of events - speech, music, syllables, sentences, etc\n",
    "* Compare amplitude of speech versus music responses in each electrode\n",
    "* Look at effects of referencing on the evoked data/epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
